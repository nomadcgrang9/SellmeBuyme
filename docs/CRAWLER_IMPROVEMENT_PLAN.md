# 크롤링 개선 플랜 (Crawler Improvement Plan)

## 📋 개요

이 문서는 셀미바이미 크롤러의 공고 수집 누락 문제를 해결하기 위한 개선 플랜입니다.

**문제 발견일**: 2026-01-20
**발견 사례**: 서울교육일자리포털에서 누리학교 공고 3건 중 1건만 수집됨
**근본 원인**: 엄격한 중복 감지 로직으로 인한 조기 종료

---

## 🔍 기존 크롤링 로직 (Before)

### 전체 흐름도

```
1. 게시판 접속
   ↓
2. 목록 페이지 HTML 로딩
   ↓
3. 공고 목록 추출 (배치 단위: 10개씩)
   ↓
4. 각 공고마다 중복 체크
   ├─ 중복이면 → 연속 중복 카운트 +1
   │              (3개 연속 중복 시 즉시 종료)
   │
   └─ 신규면 → 상세 페이지 방문
      ↓
5. 상세 페이지 스크린샷 촬영
   ↓
6. Gemini Vision API로 이미지 분석
   ↓
7. 구조화된 데이터 추출
   ↓
8. Supabase DB에 저장
   ↓
9. 다음 공고로 이동
```

### 상세 설명

#### 1단계: 게시판 접속
- **무엇을**: 크롤러가 교육청 채용공고 게시판에 접속합니다
- **예시**: `https://work.sen.go.kr` (서울교육일자리포털)
- **기술**: Playwright 브라우저 자동화

#### 2단계: 목록 페이지 HTML 로딩
- **무엇을**: 첫 페이지의 공고 목록을 화면에 불러옵니다
- **보통**: 15개씩 보기 설정 (사이트마다 다름)
- **정렬 방식**: 등록순 (최신 공고가 맨 위)

#### 3단계: 공고 목록 추출 (배치 처리)
- **무엇을**: HTML에서 공고 카드들을 찾아냅니다
- **배치 크기**: 한 번에 10개씩 처리
- **CSS 선택자**: `#srchDataDiv > ul > li` (서울 포털 기준)

**추출하는 정보:**
- 학교명 (organization)
- 공고 제목 (title)
- 지역 (location)
- 등록일 (registrationDate)
- 공고 ID (rcrtSn - URL 파라미터)

#### 4단계: 중복 체크 (Duplicate Check)
**가장 중요한 단계!** 여기서 문제가 발생했습니다.

**중복 확인 방법:**
1. 공고 URL을 DB에서 검색
2. 이미 있으면 → "중복"으로 판단
3. 없으면 → "신규"로 판단

**연속 중복 감지 (Consecutive Duplicate Detection):**
```
기존 설정:
- consecutiveDuplicateLimit: 3
- 의미: 연속으로 3개가 중복이면 "더 이상 신규 공고 없다"고 판단
- 결과: 즉시 크롤링 종료
```

**문제 시나리오 예시:**
```
[오늘 오전 10시 크롤링]
1. 동작중학교 (오늘 오전 9시 등록) → 신규 ✅
2. 강서고등학교 (어제 등록) → 중복 (1)
3. 영등포초등학교 (어제 등록) → 중복 (2)
4. 관악중학교 (어제 등록) → 중복 (3)
🛑 여기서 크롤링 종료!

5. 누리학교 (오늘 오전 8시 등록) → ❌ 수집 못 함 (이미 종료됨)
6. 성북초등학교 (오늘 오전 7시 등록) → ❌ 수집 못 함
```

**왜 이런 문제가 생기나요?**
- 서울 포털은 "등록순" 정렬 (최신이 위)
- 하지만 크롤러는 위에서 아래로 순서대로 처리
- 오늘 오전 9시에 1개 등록되면, 그 밑에는 어제 공고들이 나옴
- 어제 공고 3개 연속 만나면 → "더 이상 신규 없다"고 착각
- 하지만 실제로는 그 아래에 오늘 오전 8시, 7시 공고가 더 있음!

#### 5단계: 상세 페이지 스크린샷 촬영
**신규 공고인 경우에만 진행**

- **무엇을**: 상세 페이지 전체 화면을 이미지로 캡처
- **이유**: AI가 이미지를 보고 정보를 추출하기 위해
- **형식**: PNG 이미지 → Base64 인코딩

#### 6단계: Gemini Vision API 분석
**가장 핵심적인 AI 처리 단계**

- **무엇을**: Google Gemini AI에게 스크린샷을 보여주고 정보 추출 요청
- **비용**: 토큰당 과금 (이미지 분석은 텍스트보다 비쌈)
- **추출 정보**:
  - 학교명
  - 모집 직무
  - 채용 인원
  - 마감일
  - 급여
  - 담당 과목
  - 자격 요건

**왜 AI를 쓰나요?**
- 교육청마다 공고 양식이 제각각
- HTML 구조도 다르고, 표 형식도 다름
- AI가 이미지를 보고 "사람처럼" 이해할 수 있음

#### 7단계: 구조화된 데이터 생성
**AI가 추출한 정보를 데이터베이스 형식으로 정리**

```
AI 응답 (자연어):
"서울누리학교에서 특수교육실무사 2명을 모집합니다.
 접수 기간은 1월 19일부터 23일까지입니다."

↓ 변환 ↓

구조화된 데이터 (Structured Data):
{
  organization: "누리학교",
  title: "특수교육실무사",
  location: "동작",
  headcount: "2",
  deadline: "2026-01-23",
  ...
}
```

#### 8단계: Supabase DB 저장
**최종 데이터를 데이터베이스에 저장**

- **테이블**: `job_postings`
- **중복 방지**: `source_url`이 유일 키 (Unique Key)
- **저장 정보**: 학교명, 제목, 지역, 급여, 마감일, 태그 등

#### 9단계: 다음 공고 처리
**배치 크기(10개)만큼 반복**

- 10개 처리 완료 → 다음 배치로
- 중복률 체크: 배치 내 50% 이상 중복이면 종료
- 최대 수집 개수: 100개 (SAFETY 제한)

---

## ⚠️ 발견된 문제점

### 문제 1: 너무 엄격한 중복 감지 (주요 원인)

**설정값:**
```javascript
consecutiveDuplicateLimit: 3  // 연속 3개만 중복이면 종료
batchDuplicateThreshold: 0.5  // 배치 내 50% 중복이면 종료
```

**왜 문제인가:**
1. 서울처럼 하루 20개 이상 등록되는 곳에서는 중간에 어제 공고가 섞여 있음
2. 어제 공고 3개가 연속으로 나오면 → 바로 종료
3. 그 아래에 오늘 아침 공고들이 더 있는데 수집 못 함

**실제 사례 (누리학교):**
- 포털에 3건 있음 (rcrtSn=84124, 84125, 84296)
- DB에는 1건만 저장됨 (rcrtSn=84124)
- 2건 누락 (rcrtSn=84125, 84296)

### 문제 2: 하루 1회 크롤링 (부차적 원인)

**현재 스케줄:**
- UTC 1 AM (한국 시간 오전 10시) 1회만 실행

**잠재적 문제:**
1. 오전 10시에 크롤링
2. 오후에 20개 이상 공고 등록
3. 오전에 크롤링한 공고들이 2페이지로 밀림
4. 다음날 오전 10시 크롤링 시 1페이지만 봄
5. 2페이지로 밀린 공고는 영구 누락

**현재는 확인 안 됨:**
- 1페이지에서도 놓치는 게 더 큰 문제
- 하지만 향후 발생 가능성 높음

### 문제 3: 로그 기록 버그 (데이터 저장과 무관)

**증상:**
- DB에는 데이터 잘 저장됨
- 하지만 `crawl_logs` 테이블에는 `items_found=0`으로 기록

**원인:**
- `updateCrawlSuccess()` 함수에서 `stats` 파라미터를 INSERT에 사용 안 함
- 통계 정보가 버려짐

**영향:**
- 실제 저장에는 문제 없음
- 모니터링만 어려워짐

---

## ✅ 개선 방안

### 개선 1: SAFETY 설정 완화 (필수)

**변경 내용:**
```javascript
// Before (기존)
consecutiveDuplicateLimit: 3   // 너무 엄격
batchDuplicateThreshold: 0.5   // 50%

// After (개선)
consecutiveDuplicateLimit: 10  // 10개로 완화
batchDuplicateThreshold: 0.8   // 80%로 완화
maxItems: 150                  // 최대 수집 개수 유지
```

**효과:**
- 연속 10개까지 중복 허용 → 중간에 신규 공고 놓치지 않음
- 배치 내 80% 이상 중복일 때만 종료 → 더 많은 공고 수집

**비용 영향:**
- 중복 체크는 **무료** (DB 조회만 함)
- AI 비용은 **신규 공고에만** 발생
- 중복이 많으면 오히려 비용 절감

### 개선 2: 하루 2회 크롤링 (필수)

**변경 내용:**
```yaml
schedule:
  - cron: '0 1 * * *'  # UTC 1 AM = 한국 오전 10시
  - cron: '0 9 * * *'  # UTC 9 AM = 한국 오후 6시
```

**효과:**
1. 오전 10시: 전날 저녁~오늘 오전 공고 수집
2. 오후 6시: 오전~오후 공고 수집
3. 서로 중복 체크하므로 중복 저장 방지
4. 페이지 넘김으로 인한 누락 최소화

**비용 영향:**
- 크롤링 2배 실행
- 하지만 중복 체크로 **신규 공고만** AI 처리
- 실제 비용은 크게 증가하지 않음

### 개선 3: 로그 기록 수정 (선택)

**변경 내용:**
```javascript
// crawler/lib/supabase.js 수정
.insert({
  board_id: crawlBoardId,
  status: 'success',
  items_found: stats.jobsFound || 0,      // 추가
  items_new: stats.jobsSaved || 0,        // 추가
  items_skipped: stats.jobsSkipped || 0,  // 추가
  ai_tokens_used: stats.tokensUsed || 0,  // 추가
})
```

**효과:**
- 모니터링 정확도 향상
- 크롤링 성과 측정 가능

---

## 📊 예상 효과

### 누락 감소 (정량적)
- **기존**: 누리학교 3건 중 1건만 수집 (33% 성공률)
- **개선 후**: 3건 모두 수집 예상 (100% 성공률)
- **서울 전체**: 하루 20~30건 → 5~10건 추가 수집 예상

### 비용 영향 (정량적)
- **Gemini API 비용**:
  - 중복 체크는 무료 (DB 조회)
  - 신규 공고에만 Vision API 호출
  - 하루 2회 크롤링해도 **신규 공고 개수는 동일**
  - 예상 비용 증가: **5% 미만** (오차 범위)

### 데이터 품질 (정성적)
- 쌤찾기 서비스에 더 많은 마커 표시
- 사용자 만족도 향상
- 서울/경기/인천 타겟 시장 커버리지 증가

---

## 🚀 적용 계획

### 1단계: 코드 수정 (완료)
- ✅ `crawler/sources/seoul.js` SAFETY 설정 수정
- ✅ `.github/workflows/run-crawler.yml` 스케줄 추가

### 2단계: 로컬 테스트 (예정)
```bash
cd crawler
node index.js --source=seoul
```

**확인 사항:**
- 터미널 로그에서 수집 개수 확인
- Supabase에서 누리학교 공고 3건 모두 있는지 확인

### 3단계: 프로덕션 배포 (예정)
- Git commit & push
- GitHub Actions 자동 실행 대기
- 다음날 DB 확인

### 4단계: 모니터링 (1주일)
- 매일 서울 포털과 DB 비교
- 누락률 측정
- Gemini API 토큰 사용량 추적

---

## 📝 참고 자료

### 관련 파일
- [crawler/sources/seoul.js](../crawler/sources/seoul.js) - 서울 크롤러 로직
- [crawler/lib/supabase.js](../crawler/lib/supabase.js) - DB 저장 로직
- [.github/workflows/run-crawler.yml](../.github/workflows/run-crawler.yml) - 스케줄링

### 용어 설명
- **배치 (Batch)**: 한 번에 처리하는 공고 묶음 (10개)
- **중복 감지 (Duplicate Detection)**: 이미 DB에 있는 공고인지 확인
- **연속 중복 (Consecutive Duplicate)**: 중복이 계속 이어짐
- **Vision API**: 이미지를 보고 정보를 추출하는 AI
- **토큰 (Token)**: AI API 사용량 측정 단위

### 추가 개선 아이디어 (미정)
- 2페이지까지 크롤링 (페이지네이션 추가)
- 크롤링 실패 시 재시도 로직
- 지역별 맞춤 스케줄링 (공고 등록 패턴 분석)
