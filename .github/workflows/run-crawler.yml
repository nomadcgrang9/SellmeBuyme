name: Run Crawler

on:
  workflow_dispatch:
    inputs:
      board_id:
        description: 'Board ID to crawl'
        required: true
        type: string
      log_id:
        description: 'Crawl log ID generated by admin page'
        required: true
        type: string
      mode:
        description: 'Crawl mode (run or test)'
        required: false
        default: 'run'
        type: choice
        options:
          - run
          - test
  schedule:
    - cron: '0 1 * * *'

jobs:
  crawl-manual:
    if: github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: crawler/package-lock.json

      - name: Install dependencies
        working-directory: crawler
        run: npm ci

      - name: Cache Playwright browsers
        uses: actions/cache@v4
        with:
          path: ~/.cache/ms-playwright
          key: ${{ runner.os }}-playwright-${{ hashFiles('crawler/package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-playwright-

      - name: Install Playwright browsers
        working-directory: crawler
        run: npx playwright install chromium --with-deps

      - name: Resolve crawler source
        id: resolve_manual_source
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}
          BOARD_ID: ${{ github.event.inputs.board_id }}
        run: |
          BOARD_ID="${BOARD_ID}"
          
          # ê¸°ì¡´ í•˜ë“œì½”ë”©ëœ board ID ë§¤í•‘ (í˜¸í™˜ì„± ìœ ì§€)
          if [ "$BOARD_ID" = "f4c852f1-f49a-42c5-8823-0edd346f99bb" ]; then
            echo "source=gyeonggi" >> $GITHUB_OUTPUT
            exit 0
          elif [ "$BOARD_ID" = "5a94f47d-5feb-4821-99af-f8805cc3d619" ]; then
            echo "source=seongnam" >> $GITHUB_OUTPUT
            exit 0
          elif [ "$BOARD_ID" = "55d09cac-71aa-48d5-a8b8-bbd9181970bb" ]; then
            echo "source=uijeongbu" >> $GITHUB_OUTPUT
            exit 0
          elif [ "$BOARD_ID" = "5d7799d9-5d8d-47a2-b0df-6dd4f39449bd" ]; then
            echo "source=ai-generated" >> $GITHUB_OUTPUT
            exit 0
          elif [ "$BOARD_ID" = "a54761b2-b9c6-4011-99f8-6f0ef8dd919a" ]; then
            echo "source=incheon" >> $GITHUB_OUTPUT
            exit 0
          elif [ "$BOARD_ID" = "26ed4ae2-922f-42ef-ac38-176871668c0d" ]; then
            echo "source=seoul" >> $GITHUB_OUTPUT
            exit 0
          elif [ "$BOARD_ID" = "cf81b8f5-0902-4cb7-8e2d-06b8f8b13a56" ]; then
            echo "source=gangwon" >> $GITHUB_OUTPUT
            exit 0
          fi

          # ìƒˆë¡œìš´ AI ìƒì„± í¬ë¡¤ëŸ¬ëŠ” ê¸°ë³¸ source ì‚¬ìš©
          echo "source=ai-generated" >> $GITHUB_OUTPUT

      - name: Run crawler
        working-directory: crawler
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          CRAWL_LOG_ID: ${{ github.event.inputs.log_id }}
          CRAWL_MODE: ${{ github.event.inputs.mode }}
          CRAWLER_SOURCE: ${{ steps.resolve_manual_source.outputs.source }}
          BOARD_ID: ${{ github.event.inputs.board_id }}
        run: |
          echo "ğŸš€ Starting crawler for board: $BOARD_ID"
          echo "ğŸ“ Log ID: $CRAWL_LOG_ID"
          echo "ğŸ”§ Mode: $CRAWL_MODE"

          if [ -n "$CRAWL_LOG_ID" ]; then
            node -e "
            const { createClient } = require('@supabase/supabase-js');
            const logId = process.env.CRAWL_LOG_ID;
            const supabase = createClient(process.env.SUPABASE_URL, process.env.SUPABASE_ANON_KEY);
            if (!logId) {
              console.log('âš ï¸  Skipping log status update (no log ID)');
              process.exit(0);
            }
            supabase
              .from('crawl_logs')
              .update({ status: 'running' })
              .eq('id', logId)
              .then(() => {
                console.log('âœ… Log status updated to running');
              });
            "
          else
            echo "âš ï¸  Skipping log status update (no log ID)"
          fi

          case "$CRAWLER_SOURCE" in
            "gyeonggi")
              node index.js --source=gyeonggi
              ;;
            "seongnam")
              node index.js --source=seongnam
              ;;
            "uijeongbu")
              node index.js --source=uijeongbu
              ;;
            "incheon")
              node index.js --source=incheon
              ;;
            "seoul")
              node index.js --source=seoul
              ;;
            "gangwon")
              node index.js --source=gangwon
              ;;
            "ai-generated")
              echo "ğŸ¤– Running AI-generated crawler for board: $BOARD_ID"
              node index.js --board-id="$BOARD_ID" --mode="$CRAWL_MODE"
              ;;
            *)
              echo "âš ï¸  Unknown crawler source: $CRAWLER_SOURCE"
              exit 1
              ;;
          esac

      - name: Update log on success
        if: success()
        working-directory: crawler
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}
          CRAWL_LOG_ID: ${{ github.event.inputs.log_id }}
        run: |
          if [ -z "$CRAWL_LOG_ID" ]; then
            echo "âš ï¸  Skipping success log update (no log ID)"
            exit 0
          fi
          node -e "
          const { createClient } = require('@supabase/supabase-js');
          const logId = process.env.CRAWL_LOG_ID;
          if (!logId) {
            console.log('âš ï¸  Skipping success log update (no log ID)');
            process.exit(0);
          }
          const supabase = createClient(process.env.SUPABASE_URL, process.env.SUPABASE_ANON_KEY);
          supabase
            .from('crawl_logs')
            .update({
              status: 'success',
              completed_at: new Date().toISOString()
            })
            .eq('id', logId)
            .then(() => {
              console.log('âœ… Crawl completed successfully');
            });
          "

      - name: Update log on failure
        if: failure()
        working-directory: crawler
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}
          CRAWL_LOG_ID: ${{ github.event.inputs.log_id }}
        run: |
          if [ -z "$CRAWL_LOG_ID" ]; then
            echo "âš ï¸  Skipping failure log update (no log ID)"
            exit 0
          fi
          node -e "
          const { createClient } = require('@supabase/supabase-js');
          const logId = process.env.CRAWL_LOG_ID;
          if (!logId) {
            console.log('âš ï¸  Skipping failure log update (no log ID)');
            process.exit(0);
          }
          const supabase = createClient(process.env.SUPABASE_URL, process.env.SUPABASE_ANON_KEY);
          supabase
            .from('crawl_logs')
            .update({
              status: 'failed',
              completed_at: new Date().toISOString(),
              error_log: 'GitHub Actions workflow failed'
            })
            .eq('id', logId)
            .then(() => {
              console.log('âŒ Crawl failed');
            });
          "

  crawl-scheduled:
    if: github.event_name == 'schedule'
    runs-on: ubuntu-latest
    timeout-minutes: 30

    strategy:
      fail-fast: false
      matrix:
        board:
          - board_id: 'f4c852f1-f49a-42c5-8823-0edd346f99bb'
            source: 'gyeonggi'
            name: 'ê²½ê¸°ë„êµìœ¡ì²­ êµ¬ì¸ì •ë³´ì¡°íšŒ'
          - board_id: '5a94f47d-5feb-4821-99af-f8805cc3d619'
            source: 'seongnam'
            name: 'ì„±ë‚¨êµìœ¡ì§€ì›ì²­ êµ¬ì¸'
          - board_id: '55d09cac-71aa-48d5-a8b8-bbd9181970bb'
            source: 'uijeongbu'
            name: 'ì˜ì •ë¶€êµìœ¡ì§€ì›ì²­ êµ¬ì¸'
          - board_id: '5d7799d9-5d8d-47a2-b0df-6dd4f39449bd'
            source: 'ai-generated'
            name: 'êµ¬ë¦¬ë‚¨ì–‘ì£¼ ê¸°ê°„ì œêµì‚¬'
          - board_id: 'de02eada-6569-45df-9f4d-45a4fcc51879'
            source: 'ai-generated'
            name: 'ê°€í‰êµìœ¡ì§€ì›ì²­ ê¸°ê°„ì œêµì› êµ¬ì¸êµ¬ì§'
          - board_id: 'a54761b2-b9c6-4011-99f8-6f0ef8dd919a'
            source: 'incheon'
            name: 'ì¸ì²œêµìœ¡ì²­ ì±„ìš©ê³µê³ '
          - board_id: '26ed4ae2-922f-42ef-ac38-176871668c0d'
            source: 'seoul'
            name: 'ì„œìš¸êµìœ¡ì¼ìë¦¬í¬í„¸'
          - board_id: 'cf81b8f5-0902-4cb7-8e2d-06b8f8b13a56'
            source: 'gangwon'
            name: 'ê°•ì›íŠ¹ë³„ìì¹˜ë„êµìœ¡ì²­ í•™êµì¸ë ¥ì±„ìš©'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: crawler/package-lock.json

      - name: Install dependencies
        working-directory: crawler
        run: npm ci

      - name: Cache Playwright browsers
        uses: actions/cache@v4
        with:
          path: ~/.cache/ms-playwright
          key: ${{ runner.os }}-playwright-${{ hashFiles('crawler/package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-playwright-

      - name: Install Playwright browsers
        working-directory: crawler
        run: npx playwright install chromium --with-deps

      - name: Run crawler
        shell: bash
        working-directory: crawler
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          CRAWL_LOG_ID: ''
          CRAWL_MODE: 'run'
          CRAWLER_SOURCE: ${{ matrix.board.source }}
          BOARD_ID: ${{ matrix.board.board_id }}
          BOARD_NAME: ${{ matrix.board.name }}
        run: |
          echo "ğŸš€ Starting crawler for board: $BOARD_ID ($BOARD_NAME)"
          echo "ğŸ”§ Mode: $CRAWL_MODE"

          case "$CRAWLER_SOURCE" in
            "gyeonggi")
              node index.js --source=gyeonggi
              ;;
            "seongnam")
              node index.js --source=seongnam
              ;;
            "uijeongbu")
              node index.js --source=uijeongbu
              ;;
            "incheon")
              node index.js --source=incheon
              ;;
            "seoul")
              node index.js --source=seoul
              ;;
            "gangwon")
              node index.js --source=gangwon
              ;;
            "ai-generated")
              echo "ğŸ¤– Running AI-generated crawler for board: $BOARD_ID"
              node index.js --board-id="$BOARD_ID" --mode="$CRAWL_MODE"
              ;;
            *)
              echo "âš ï¸  Unknown crawler source: $CRAWLER_SOURCE"
              exit 1
              ;;
          esac

      - name: Update log on success
        if: success()
        working-directory: crawler
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}
          CRAWL_LOG_ID: ''
        run: |
          echo "â„¹ï¸  Scheduled run: no crawl log to update"

      - name: Update log on failure
        if: failure()
        working-directory: crawler
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}
          CRAWL_LOG_ID: ''
        run: |
          echo "â„¹ï¸  Scheduled run: no crawl log to update"
