name: Run Crawler

on:
  workflow_dispatch:
    inputs:
      board_id:
        description: 'Board ID to crawl'
        required: true
        type: string
      log_id:
        description: 'Crawl log ID generated by admin page'
        required: true
        type: string
      mode:
        description: 'Crawl mode (run or test)'
        required: false
        default: 'run'
        type: choice
        options:
          - run
          - test
  schedule:
    - cron: '0 1 * * *'

jobs:
  crawl-manual:
    if: github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: crawler/package-lock.json

      - name: Install dependencies
        working-directory: crawler
        run: npm ci

      - name: Install Playwright browsers
        working-directory: crawler
        run: npx playwright install chromium --with-deps

      - name: Resolve crawler source
        id: resolve_manual_source
        run: |
          BOARD_ID="${{ github.event.inputs.board_id }}"
          if [ "$BOARD_ID" = "f4c852f1-f49a-42c5-8823-0edd346f99bb" ]; then
            echo "source=gyeonggi" >> $GITHUB_OUTPUT
          elif [ "$BOARD_ID" = "5a94f47d-5feb-4821-99af-f8805cc3d619" ]; then
            echo "source=seongnam" >> $GITHUB_OUTPUT
          elif [ "$BOARD_ID" = "55d09cac-71aa-48d5-a8b8-bbd9181970bb" ]; then
            echo "source=uijeongbu" >> $GITHUB_OUTPUT
          else
            echo "Unknown board ID: $BOARD_ID"
            exit 1
          fi

      - name: Run crawler
        working-directory: crawler
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          CRAWL_LOG_ID: ${{ github.event.inputs.log_id }}
          CRAWL_MODE: ${{ github.event.inputs.mode }}
          CRAWLER_SOURCE: ${{ steps.resolve_manual_source.outputs.source }}
          BOARD_ID: ${{ github.event.inputs.board_id }}
        run: |
          echo "üöÄ Starting crawler for board: $BOARD_ID"
          echo "üìù Log ID: $CRAWL_LOG_ID"
          echo "üîß Mode: $CRAWL_MODE"

          if [ -n "$CRAWL_LOG_ID" ]; then
            node -e "
            const { createClient } = require('@supabase/supabase-js');
            const logId = process.env.CRAWL_LOG_ID;
            const supabase = createClient(process.env.SUPABASE_URL, process.env.SUPABASE_ANON_KEY);
            if (!logId) {
              console.log('‚ö†Ô∏è  Skipping log status update (no log ID)');
              process.exit(0);
            }
            supabase
              .from('crawl_logs')
              .update({ status: 'running' })
              .eq('id', logId)
              .then(() => {
                console.log('‚úÖ Log status updated to running');
              });
            "
          else
            echo "‚ö†Ô∏è  Skipping log status update (no log ID)"
          fi

          case "$CRAWLER_SOURCE" in
            "gyeonggi")
              node index.js --source=gyeonggi
              ;;
            "seongnam")
              node index.js --source=seongnam
              ;;
            "uijeongbu")
              node index.js --source=uijeongbu
              ;;
            *)
              echo "‚ö†Ô∏è  Unknown crawler source: $CRAWLER_SOURCE"
              exit 1
              ;;
          esac

      - name: Update log on success
        if: success()
        working-directory: crawler
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}
          CRAWL_LOG_ID: ${{ github.event.inputs.log_id }}
        run: |
          if [ -z "$CRAWL_LOG_ID" ]; then
            echo "‚ö†Ô∏è  Skipping success log update (no log ID)"
            exit 0
          fi
          node -e "
          const { createClient } = require('@supabase/supabase-js');
          const logId = process.env.CRAWL_LOG_ID;
          if (!logId) {
            console.log('‚ö†Ô∏è  Skipping success log update (no log ID)');
            process.exit(0);
          }
          const supabase = createClient(process.env.SUPABASE_URL, process.env.SUPABASE_ANON_KEY);
          supabase
            .from('crawl_logs')
            .update({
              status: 'success',
              completed_at: new Date().toISOString()
            })
            .eq('id', logId)
            .then(() => {
              console.log('‚úÖ Crawl completed successfully');
            });
          "

      - name: Update log on failure
        if: failure()
        working-directory: crawler
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}
          CRAWL_LOG_ID: ${{ github.event.inputs.log_id }}
        run: |
          if [ -z "$CRAWL_LOG_ID" ]; then
            echo "‚ö†Ô∏è  Skipping failure log update (no log ID)"
            exit 0
          fi
          node -e "
          const { createClient } = require('@supabase/supabase-js');
          const logId = process.env.CRAWL_LOG_ID;
          if (!logId) {
            console.log('‚ö†Ô∏è  Skipping failure log update (no log ID)');
            process.exit(0);
          }
          const supabase = createClient(process.env.SUPABASE_URL, process.env.SUPABASE_ANON_KEY);
          supabase
            .from('crawl_logs')
            .update({
              status: 'failed',
              completed_at: new Date().toISOString(),
              error_log: 'GitHub Actions workflow failed'
            })
            .eq('id', logId)
            .then(() => {
              console.log('‚ùå Crawl failed');
            });
          "

  crawl-scheduled:
    if: github.event_name == 'schedule'
    runs-on: ubuntu-latest
    timeout-minutes: 30

    strategy:
      matrix:
        board:
          - board_id: 'f4c852f1-f49a-42c5-8823-0edd346f99bb'
            source: 'gyeonggi'
            name: 'Í≤ΩÍ∏∞ÎèÑÍµêÏú°Ï≤≠ Íµ¨Ïù∏Ï†ïÎ≥¥Ï°∞Ìöå'
          - board_id: '5a94f47d-5feb-4821-99af-f8805cc3d619'
            source: 'seongnam'
            name: 'ÏÑ±ÎÇ®ÍµêÏú°ÏßÄÏõêÏ≤≠ Íµ¨Ïù∏'
          - board_id: '55d09cac-71aa-48d5-a8b8-bbd9181970bb'
            source: 'uijeongbu'
            name: 'ÏùòÏ†ïÎ∂ÄÍµêÏú°ÏßÄÏõêÏ≤≠ Íµ¨Ïù∏'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: crawler/package-lock.json

      - name: Install dependencies
        working-directory: crawler
        run: npm ci

      - name: Install Playwright browsers
        working-directory: crawler
        run: npx playwright install chromium --with-deps

      - name: Run crawler
        working-directory: crawler
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          CRAWL_LOG_ID: ''
          CRAWL_MODE: 'run'
          CRAWLER_SOURCE: ${{ matrix.board.source }}
          BOARD_ID: ${{ matrix.board.board_id }}
        run: |
          echo "üöÄ Starting crawler for board: $BOARD_ID (${matrix.board.name})"
          echo "üîß Mode: $CRAWL_MODE"

          case "$CRAWLER_SOURCE" in
            "gyeonggi")
              node index.js --source=gyeonggi
              ;;
            "seongnam")
              node index.js --source=seongnam
              ;;
            "uijeongbu")
              node index.js --source=uijeongbu
              ;;
            *)
              echo "‚ö†Ô∏è  Unknown crawler source: $CRAWLER_SOURCE"
              exit 1
              ;;
          esac

      - name: Update log on success
        if: success()
        working-directory: crawler
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}
          CRAWL_LOG_ID: ''
        run: |
          echo "‚ÑπÔ∏è  Scheduled run: no crawl log to update"

      - name: Update log on failure
        if: failure()
        working-directory: crawler
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}
          CRAWL_LOG_ID: ''
        run: |
          echo "‚ÑπÔ∏è  Scheduled run: no crawl log to update"
