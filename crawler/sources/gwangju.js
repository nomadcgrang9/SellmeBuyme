import { chromium } from 'playwright';
import { supabase } from '../lib/supabase.js';
import { fileURLToPath } from 'url';

const config = {
    name: "ê´‘ì£¼ê´‘ì—­ì‹œêµìœ¡ì²­",
    baseUrl: "https://www.gen.go.kr/xboard/board.php?tbnum=32",
    region: "ê´‘ì£¼",
};

function getCutoffDate() {
    const today = new Date();
    today.setHours(0, 0, 0, 0);
    const mode = process.env.CRAWL_MODE || 'initial';
    const daysToSubtract = (mode === 'daily') ? 1 : 2;
    const cutoffDate = new Date(today);
    cutoffDate.setDate(today.getDate() - daysToSubtract);
    return cutoffDate;
}

export async function crawl() {
    console.log(`\nğŸ“ ${config.name} í¬ë¡¤ë§ ì‹œì‘`);
    const browser = await chromium.launch({ headless: true });
    const context = await browser.newContext({ userAgent: 'Mozilla/5.0' });
    const page = await context.newPage();

    // í—¤ë” ì„¤ì • (ë´‡ íƒì§€ ë°©ì§€)
    await page.setExtraHTTPHeaders({
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7'
    });

    let jobs = [];

    try {
        const cutoffDate = getCutoffDate();
        console.log(`ğŸ“… ìˆ˜ì§‘ ê¸°ì¤€: ${cutoffDate.toISOString().split('T')[0]} ì´í›„ ë°ì´í„°`);

        // Phase 1: ëª©ë¡ ìˆ˜ì§‘
        const collectedItems = [];
        let stopCrawling = false;
        let pageNum = 1;
        const maxPages = 10;

        while (!stopCrawling && pageNum <= maxPages) {
            console.log(`ğŸ“„ ëª©ë¡ í˜ì´ì§€ ${pageNum} ì ‘ê·¼ ì¤‘...`);
            const listUrl = `${config.baseUrl}&page=${pageNum}`;
            await page.goto(listUrl, { waitUntil: 'domcontentloaded', timeout: 30000 });

            const rows = await page.$$('table tbody tr');
            if (rows.length === 0) break;

            let validItemsInPage = 0;

            for (const row of rows) {
                const columns = await row.$$('td');
                if (columns.length < 5) continue;

                const numText = await columns[0].textContent().then(t => t.trim());
                const titleText = await columns[2].innerText().then(t => t.trim());
                const dateText = await columns[4].textContent().then(t => t.trim()); // 2025.01.02
                const linkEl = await columns[2].$('a');

                if (!titleText || !linkEl) continue;
                const linkHref = await linkEl.getAttribute('href');

                // ë‚ ì§œ íŒŒì‹±
                let postDate = null;
                const dateParts = dateText.split('.');
                if (dateParts.length === 3) {
                    postDate = new Date(`${dateParts[0]}-${dateParts[1]}-${dateParts[2]}`);
                    postDate.setHours(0, 0, 0, 0);
                }

                const isNotice = numText === 'ê³µì§€';

                // ë‚ ì§œ í•„í„°ë§
                if (postDate) {
                    if (postDate < cutoffDate) {
                        if (isNotice) continue;
                        stopCrawling = true;
                        console.log(`  ğŸ›‘ ë‚ ì§œ ì œí•œ ë„ë‹¬ (${dateText})`);
                        continue;
                    }
                }

                // ë§í¬ ì ˆëŒ€ê²½ë¡œ ë³€í™˜
                let fullLink = linkHref;
                if (linkHref && !linkHref.startsWith('http')) {
                    fullLink = new URL(linkHref, config.baseUrl).href;
                }

                collectedItems.push({
                    title: titleText,
                    date: dateText.replace(/\./g, '-'),
                    link: fullLink,
                    schoolName: "ê´‘ì£¼ê´‘ì—­ì‹œêµìœ¡ì²­",
                });
                validItemsInPage++;
            }

            if (validItemsInPage === 0 && stopCrawling) break;
            pageNum++;
        }

        console.log(`âœ… Phase 1 ì™„ë£Œ: ì´ ${collectedItems.length}ê°œ ë§í¬ ì‹ë³„`);

        // Phase 2: ìƒì„¸ ìˆ˜ì§‘
        for (const item of collectedItems) {
            console.log(`  ğŸ” ìƒì„¸ í¬ë¡¤ë§: ${item.title}`);
            try {
                const detailData = await crawlDetailPage(page, item.link);
                jobs.push({
                    ...item,
                    ...detailData,
                    location: config.region
                });
                await page.waitForTimeout(500);
            } catch (e) {
                console.error(`  âš ï¸ ìƒì„¸ ìˆ˜ì§‘ ì‹¤íŒ¨ (${item.title}): ${e.message}`);
            }
        }

    } catch (error) {
        console.error(`âŒ í¬ë¡¤ë§ ì¹˜ëª…ì  ì˜¤ë¥˜: ${error.message}`);
    } finally {
        await browser.close();
    }

    return jobs;
}

async function crawlDetailPage(page, url) {
    await page.goto(url, { waitUntil: 'domcontentloaded' });

    const content = await page.evaluate(() => {
        const el = document.querySelector('#board_view') || document.querySelector('.board_view');
        return el ? el.innerText.trim() : '';
    });

    const attachments = await page.evaluate(() => {
        const links = Array.from(document.querySelectorAll('.file_down a, a[href*="download"]'));
        return links.map(a => ({
            name: a.innerText.trim(),
            url: a.href
        })).filter(f => f.url && f.name);
    });

    return {
        detailContent: content,
        attachments: attachments,
        attachmentUrl: attachments.length > 0 ? attachments[0].url : null,
        attachmentFilename: attachments.length > 0 ? attachments[0].name : null,
    };
}

if (process.argv[1] === fileURLToPath(import.meta.url)) {
    (async () => {
        const results = await crawl();
        if (results.length > 0) {
            const { error } = await supabase.from('job_postings').upsert(results, { onConflict: 'link' });
            if (error) console.error('DB Save Failed:', error);
            else console.log(`âœ… Saved ${results.length} items to DB`);
        } else {
            console.log('No items found to save.');
        }
    })();
}
