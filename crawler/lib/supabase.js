import { createClient } from '@supabase/supabase-js';
import dotenv from 'dotenv';

dotenv.config();

// VITE_ prefix ë³€ìˆ˜ë“¤ë„ ì§€ì› (í”„ë¡ íŠ¸ì—”ë“œì™€ ë™ì¼í•œ .env ì‚¬ìš©)
const supabaseUrl = process.env.SUPABASE_URL || process.env.VITE_SUPABASE_URL;
// SERVICE_ROLE_KEYë¥¼ ìš°ì„  ì‚¬ìš© (RLS ìš°íšŒ), ì—†ìœ¼ë©´ ANON_KEY í´ë°±
const supabaseKey = process.env.SUPABASE_SERVICE_ROLE_KEY || process.env.SUPABASE_ANON_KEY || process.env.VITE_SUPABASE_ANON_KEY;

if (!supabaseUrl || !supabaseKey) {
  throw new Error('Supabase credentials not found in .env file. Required: SUPABASE_URL (or VITE_SUPABASE_URL) and SUPABASE_SERVICE_ROLE_KEY');
}

export const supabase = createClient(supabaseUrl, supabaseKey);

// í¬ë¡¤ë§ ì†ŒìŠ¤ ì •ë³´ ì¡°íšŒ ë˜ëŠ” ìƒì„±
export async function getOrCreateCrawlSource(config) {
  const { name, baseUrl, region, isLocalGovernment } = config;

  // 1. URLë¡œ ë¨¼ì € ê²€ìƒ‰ (ì´ë¯¸ ë“±ë¡ëœ URLì¸ì§€ í™•ì¸)
  let { data: board } = await supabase
    .from('crawl_boards')
    .select('id, crawl_batch_size, region, is_local_government, name')
    .eq('board_url', baseUrl)
    .maybeSingle();

  // 2. ì—†ìœ¼ë©´ ì´ë¦„ìœ¼ë¡œ ê²€ìƒ‰
  if (!board) {
    const { data: boardByName } = await supabase
      .from('crawl_boards')
      .select('id, crawl_batch_size, region, is_local_government, name')
      .eq('name', name)
      .maybeSingle();
    board = boardByName;
  }

  if (board) {
    console.log(`âœ… ê¸°ì¡´ ê²Œì‹œíŒ ì‚¬ìš©: ${board.name} (ID: ${board.id})`);
    return {
      id: board.id,
      crawlBatchSize: board.crawl_batch_size ?? 10,
      region: board.region,
      isLocalGovernment: board.is_local_government,
    };
  }

  // ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
  console.log(`âœ¨ ìƒˆ ê²Œì‹œíŒ ìƒì„± ì¤‘: ${name}`);
  const { data: newBoard, error } = await supabase
    .from('crawl_boards')
    .insert({
      name: name,
      board_url: baseUrl,
      region: region || 'ê¸°íƒ€',
      is_local_government: isLocalGovernment || false,
      crawl_batch_size: 10,
      status: 'active'
    })
    .select()
    .single();

  if (error) {
    throw new Error(`ê²Œì‹œíŒ ìƒì„± ì‹¤íŒ¨: ${error.message}`);
  }

  return {
    id: newBoard.id,
    crawlBatchSize: newBoard.crawl_batch_size,
    region: newBoard.region,
    isLocalGovernment: newBoard.is_local_government
  };
}

/**
 * ê³µê³  ì¤‘ë³µ ì²´í¬ (source_url ê¸°ì¤€)
 */
export async function getExistingJobBySource(sourceUrl) {
  const { data, error } = await supabase
    .from('job_postings')
    .select('*')
    .eq('source_url', sourceUrl)
    .single();

  if (error && error.code !== 'PGRST116') {
    console.warn(`âš ï¸  ê¸°ì¡´ ê³µê³  ì¡°íšŒ ì‹¤íŒ¨: ${error.message}`);
  }

  return data;
}

/**
 * ê³µê³  ë°ì´í„° ì €ì¥
 */
export async function saveJobPosting(jobData, crawlSourceId, hasContentImages = false) {
  // ë³¸ë¬¸ ê¸¸ì´ ê²€ì¦ (300ì ë¯¸ë§Œì´ê³  ë³¸ë¬¸ ì´ë¯¸ì§€ë„ ì—†ê³  ì²¨ë¶€íŒŒì¼ë„ ì—†ìœ¼ë©´ ì •ë³´ ë¶€ì¡±ìœ¼ë¡œ ê°„ì£¼)
  // ì²¨ë¶€íŒŒì¼ì´ ìˆëŠ” ê²½ìš° ë³¸ë¬¸ ê¸¸ì´ ì¡°ê±´ ì™„í™” (ì²¨ë¶€íŒŒì¼ì— ìƒì„¸ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°ê°€ ë§ìŒ)
  const contentLength = (jobData.detail_content || '').trim().length;
  const hasAttachment = !!(jobData.attachment_url);

  if (contentLength < 300 && !hasContentImages && !hasAttachment) {
    console.warn(`âš ï¸  ë³¸ë¬¸ ê¸¸ì´ ë¶€ì¡± & ì´ë¯¸ì§€/ì²¨ë¶€íŒŒì¼ ì—†ìŒìœ¼ë¡œ ì €ì¥ ê±´ë„ˆëœ€: ${jobData.title} (${contentLength}ì)`);
    return null;
  }

  // ì²¨ë¶€íŒŒì¼ë§Œ ìˆê³  ë³¸ë¬¸ì´ ë§¤ìš° ì§§ì€ ê²½ìš° ë¡œê¹… (ì •ë³´ ì°¸ê³ ìš©)
  if (contentLength < 300 && hasAttachment) {
    console.log(`ğŸ“ ë³¸ë¬¸ ì§§ì§€ë§Œ ì²¨ë¶€íŒŒì¼ ìˆìŒ - ì €ì¥ ì§„í–‰: ${jobData.title} (${contentLength}ì)`);
  }

  const existing = await getExistingJobBySource(jobData.source_url);

  const payload = {
    source: 'crawled',
    crawl_board_id: crawlSourceId,  // ìˆ˜ì •: crawl_source_id â†’ crawl_board_id (DB ìŠ¤í‚¤ë§ˆì™€ ì¼ì¹˜)
    organization: jobData.organization,
    title: jobData.title,
    job_type: jobData.job_type,
    content: jobData.detail_content,
    detail_content: jobData.detail_content,
    tags: jobData.tags || [],
    location: jobData.location,
    compensation: jobData.compensation,
    deadline: jobData.deadline,
    is_urgent: jobData.is_urgent || false,
    source_url: jobData.source_url,
    attachment_url: jobData.attachment_url,
    application_period: jobData.application_period,
    work_period: jobData.work_period,
    work_time: jobData.work_time,
    contact: jobData.contact,
    qualifications: jobData.qualifications || [],
    structured_content: jobData.structured_content,
    school_level: jobData.school_level,
    subject: jobData.subject,
    required_license: jobData.required_license,
  };

  if (existing) {
    const { data: updated, error: updateError } = await supabase
      .from('job_postings')
      .update({
        ...payload,
        updated_at: new Date().toISOString(),
      })
      .eq('id', existing.id)
      .select()
      .maybeSingle();

    if (updateError) {
      console.error(`âŒ ê³µê³  ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: ${updateError.message}`);
      return null;
    }

    if (!updated) {
      console.warn(`âš ï¸  ì—…ë°ì´íŠ¸ ëŒ€ìƒ ì—†ìŒ: ${jobData.title}`);
      return null;
    }

    console.log(`â™»ï¸  ê¸°ì¡´ ê³µê³  ì—…ë°ì´íŠ¸: ${jobData.title}`);
    return updated;
  }

  const { data, error } = await supabase
    .from('job_postings')
    .insert(payload)
    .select()
    .single();

  if (error) {
    console.error(`âŒ ì €ì¥ ì‹¤íŒ¨: ${error.message}`);
    return null;
  }

  console.log(`âœ… ì €ì¥ ì™„ë£Œ: ${jobData.title}`);
  return data;
}

/**
 * í¬ë¡¤ë§ ì„±ê³µ ì‹œê°„ ì—…ë°ì´íŠ¸
 */
export async function updateCrawlSuccess(crawlSourceId) {
  await supabase
    .from('crawl_sources')
    .update({
      last_successful: new Date().toISOString(),
      error_count: 0
    })
    .eq('id', crawlSourceId);
}

/**
 * í¬ë¡¤ë§ ì‹¤íŒ¨ ì¹´ìš´íŠ¸ ì¦ê°€
 */
export async function incrementErrorCount(crawlSourceId) {
  await supabase.rpc('increment_error_count', { source_id: crawlSourceId });
}
